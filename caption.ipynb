{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captioning / Scene Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from datetime import timedelta\n",
    "from os import listdir, makedirs, path\n",
    "\n",
    "from PIL import Image as PImage\n",
    "\n",
    "VIDEO_DB_PATH = \"./metadata/keyframe-500/videos.json\"\n",
    "OUT_PATH = \"./metadata/caption-1152\"\n",
    "makedirs(OUT_PATH, exist_ok=True)\n",
    "\n",
    "VIDEO_PATH = \"../../vids/0801-500\"\n",
    "DIR_PATTERN = re.compile(\"^[0-3][0-9]-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Video Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VIDEO_DB_PATH, \"r\") as f:\n",
    "  video_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration, BlipForQuestionAnswering\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "\n",
    "MODEL_NAMES = [\"Salesforce/blip-image-captioning-large\", \"nlpconnect/vit-gpt2-image-captioning\"]\n",
    "\n",
    "CAP_MODELS = [\n",
    "  {\n",
    "    \"model\": BlipForConditionalGeneration.from_pretrained(MODEL_NAMES[0]).to(\"cuda\"),\n",
    "    \"pre\": BlipProcessor.from_pretrained(MODEL_NAMES[0]),\n",
    "    \"conditional\": \"image of\"\n",
    "  },\n",
    "  {\n",
    "    \"model\": VisionEncoderDecoderModel.from_pretrained(MODEL_NAMES[1]).to(\"cuda\"),\n",
    "    \"pre\": ViTImageProcessor.from_pretrained(MODEL_NAMES[1]),\n",
    "    \"post\": AutoTokenizer.from_pretrained(MODEL_NAMES[1])\n",
    "  }\n",
    "]\n",
    "\n",
    "for m in CAP_MODELS:\n",
    "  m[\"post\"] = m.get(\"post\", m[\"pre\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "QA_MODEL_NAME = \"openbmb/MiniCPM-V-2\"\n",
    "\n",
    "qa_model = AutoModel.from_pretrained(QA_MODEL_NAME, trust_remote_code=True, torch_dtype=torch.bfloat16).to(\"cuda\", dtype=torch.bfloat16)\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(QA_MODEL_NAME, trust_remote_code=True)\n",
    "_ = qa_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OSOI = [\n",
    "  \"people\",\n",
    "  \"police officers\",\n",
    "  \"protesters\",\n",
    "  \"cars\",\n",
    "  \"flags\",\n",
    "  \"tables\",\n",
    "  \"chairs\",\n",
    "  \"mirrors\",\n",
    "  \"windows\",\n",
    "  \"doors\",\n",
    "  \"ramps\",\n",
    "  \"stairs\",\n",
    "  \"elevators\",\n",
    "  \"support columns\",\n",
    "  \"paintings\",\n",
    "  \"statues\"\n",
    "]\n",
    "questions = [f\"are {obj} in the image?\" for obj in OSOI]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "input_dirs = sorted([d for d in listdir(VIDEO_PATH) if DIR_PATTERN.search(d) is not None])\n",
    "mLang = \"en\"\n",
    "\n",
    "for io_dir in input_dirs[:1]:\n",
    "  output_dir_path = path.join(OUT_PATH, io_dir)\n",
    "  makedirs(output_dir_path, exist_ok=True)\n",
    "\n",
    "  input_dir_path = path.join(VIDEO_PATH, io_dir)\n",
    "  input_files = sorted([f for f in listdir(input_dir_path) if f.endswith(\"mp4\")])\n",
    "\n",
    "  for io_file in input_files:\n",
    "    input_file_path = path.join(input_dir_path, io_file)\n",
    "    output_file_path = path.join(output_dir_path, io_file.replace(\".mp4\", \".json\"))\n",
    "\n",
    "    if io_file not in video_data:\n",
    "      print(io_file, \"not in video_data\")\n",
    "      continue\n",
    "\n",
    "    if path.isfile(output_file_path):\n",
    "      continue\n",
    "\n",
    "    print(io_dir, io_file)\n",
    "\n",
    "    vid = cv2.VideoCapture(input_file_path)\n",
    "    vw = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    vh = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_count = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(vid.get(cv2.CAP_PROP_FPS))\n",
    "    rep_frames = video_data[io_file][\"representative_frames\"]\n",
    "    print(len(rep_frames))\n",
    "\n",
    "    cap_languages = set(video_data[io_file].get(\"caption_languages\", []))\n",
    "    cap_languages.add(mLang)\n",
    "    video_data[io_file][\"caption_languages\"] = list(cap_languages)\n",
    "\n",
    "    vid.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    for frame_data in rep_frames:\n",
    "      frame_data[\"captions\"] = frame_data.get(\"captions\", {})\n",
    "      frame_data[\"captions\"][mLang] = []\n",
    "\n",
    "      frameIdx = frame_data[\"index\"]\n",
    "      vid.set(cv2.CAP_PROP_POS_FRAMES, frameIdx)\n",
    "      _, frame = vid.read()\n",
    "      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "      image = PImage.fromarray(frame)\n",
    "\n",
    "      for m in CAP_MODELS:\n",
    "        if \"conditional\" in m:\n",
    "          input = m[\"pre\"](image, m[\"conditional\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "          cap_offset = len(m[\"conditional\"]) + 1\n",
    "        else:\n",
    "          input = m[\"pre\"](image, return_tensors=\"pt\").to(\"cuda\")\n",
    "          cap_offset = 0\n",
    "        cap_out = m[\"model\"].generate(**input, max_length=50)\n",
    "        caption = m[\"post\"].decode(cap_out[0], skip_special_tokens=True)\n",
    "        frame_data[\"captions\"][mLang].append(caption[cap_offset:])\n",
    "\n",
    "      msgs = [\n",
    "        {'role': 'user', 'content': \"The following image was taken during a protest.\"},\n",
    "        {'role': 'user', 'content': \"Give a short description of the image.\"},\n",
    "        {'role': 'user', 'content': \"Don't mention sports or winter.\"},\n",
    "      ]\n",
    "\n",
    "      caption, _, _ = qa_model.chat(\n",
    "        image=image,\n",
    "        msgs=msgs,\n",
    "        max_length=32,\n",
    "        context=None,\n",
    "        tokenizer=qa_tokenizer,\n",
    "        sampling=True,\n",
    "        temperature=0.1\n",
    "      )\n",
    "      caption = caption[:caption.find(\".\") + 1]\n",
    "      caption = caption[:caption.find(\", possibly\")]\n",
    "      frame_data[\"captions\"][mLang].append(caption)\n",
    "\n",
    "      answers = []\n",
    "      for o in OSOI:\n",
    "        question = f'using only yes or no, are there any {o} in the image?'\n",
    "        msgs = [{'role': 'user', 'content': question}]\n",
    "        res, _, _ = qa_model.chat(\n",
    "          image=image,\n",
    "          msgs=msgs,\n",
    "          context=None,\n",
    "          tokenizer=qa_tokenizer,\n",
    "          sampling=True,\n",
    "          temperature=0.1\n",
    "        )\n",
    "        answers.append(res.split(',')[0].lower())\n",
    "\n",
    "      frame_data[\"objects\"] = [o for o,a in zip(OSOI, answers) if a == \"yes\"]\n",
    "\n",
    "    with open(output_file_path, \"w\") as of:\n",
    "      json.dump(video_data[io_file][\"representative_frames\"], of, sort_keys=True, indent=2, separators=(',',':'))\n",
    "\n",
    "    vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Process: add to metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from os import listdir, path\n",
    "\n",
    "VIDEO_DB_PATH_IN = \"./metadata/keyframe-500/videos.json\"\n",
    "\n",
    "CAPTION_PATH = \"./metadata/caption-1152\"\n",
    "VIDEO_DB_PATH_OUT = path.join(CAPTION_PATH, \"videos.json\")\n",
    "\n",
    "DIR_PATTERN = re.compile(\"^[0-3][0-9]-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open all caption files\n",
    "cap_data = {}\n",
    "\n",
    "input_dirs = sorted([d for d in listdir(CAPTION_PATH) if DIR_PATTERN.search(d) is not None])\n",
    "\n",
    "for io_dir in input_dirs:\n",
    "  input_dir_path = path.join(CAPTION_PATH, io_dir)\n",
    "  input_files = sorted([f for f in listdir(input_dir_path) if f.endswith(\"json\")])\n",
    "\n",
    "  for io_file in input_files:\n",
    "    input_file_path = path.join(input_dir_path, io_file)\n",
    "    video_key = io_file.replace(\"json\", \"mp4\")\n",
    "    with open(input_file_path, \"r\") as f:\n",
    "      cap_data[video_key] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VIDEO_DB_PATH_IN, \"r\") as f:\n",
    "  video_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, vdata in video_data.items():\n",
    "  if k not in cap_data:\n",
    "    print(k, \"has no caption info\")\n",
    "  else:\n",
    "    video_data[k][\"representative_frames\"] = cap_data[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VIDEO_DB_PATH_OUT, \"w\") as f:\n",
    "  json.dump(video_data, f, indent=2, separators=(',',':'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Process: create objects json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from os import path\n",
    "\n",
    "CAPTION_PATH = \"./metadata/caption-1152\"\n",
    "VIDEO_DB_PATH_IN = path.join(CAPTION_PATH, \"videos.json\")\n",
    "OBJ_PATH_OUT = path.join(CAPTION_PATH, \"objects.json\")\n",
    "\n",
    "with open(VIDEO_DB_PATH_IN, \"r\") as f:\n",
    "  video_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp(pos, seek):\n",
    "  lt = [[ts,s] for ts,s in seek if s < pos]\n",
    "  gt = [[ts,s] for ts,s in seek if s > pos]\n",
    "  ts0, pos0 = lt[-1]\n",
    "  ts1, pos1 = gt[0]\n",
    "  return int(((pos - pos0) / (pos1 - pos0)) * (ts1 - ts0) + ts0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files/Frames/Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = []\n",
    "frame_data = []\n",
    "obj_data = {}\n",
    "\n",
    "for vid,data in video_data.items():\n",
    "  m_path = f\"{data['camera']}/{data['name']}\"\n",
    "  m_file_key = len(file_data)\n",
    "  file_data.append(m_path)\n",
    "  for f in data[\"representative_frames\"]:\n",
    "    m_frame = f[\"index\"]\n",
    "    m_pos = f[\"index\"] / data[\"fps\"]\n",
    "    m_timestamp = get_timestamp(m_pos, data[\"seek\"])\n",
    "    m_caption = f[\"captions\"][\"en\"][-1].replace(\"The image shows \", \"\")\n",
    "    m_frame_key = len(frame_data)\n",
    "    frame_data.append({\n",
    "      \"file\": m_file_key,\n",
    "      \"frame\": m_frame,\n",
    "      \"time\": m_pos,\n",
    "      \"timestamp\": m_timestamp,\n",
    "      \"caption\": m_caption\n",
    "    })\n",
    "\n",
    "    for o in f[\"objects\"]:\n",
    "      if o not in obj_data:\n",
    "        obj_data[o] = []\n",
    "      obj_data[o].append({\n",
    "        \"frame\": m_frame_key,\n",
    "        \"timestamp\": m_timestamp\n",
    "      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in obj_data.items():\n",
    "  sorted_by_ts = sorted(v, key=lambda x: x[\"timestamp\"])\n",
    "  obj_data[k] = [x[\"frame\"] for x in sorted_by_ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data = {\n",
    "  \"files\": file_data,\n",
    "  \"frames\": frame_data,\n",
    "  \"objects\": obj_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OBJ_PATH_OUT, \"w\") as f:\n",
    "  json.dump(out_data, f, separators=(',',':'), sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_data = {}\n",
    "\n",
    "for k,data in video_data.items():\n",
    "  for f in data[\"representative_frames\"]:\n",
    "    m_frame = f[\"index\"]\n",
    "    m_pos = f[\"index\"] / data[\"fps\"]\n",
    "    m_timestamp = get_timestamp(m_pos, data[\"seek\"])\n",
    "\n",
    "    for o in f[\"objects\"]:\n",
    "      if o not in obj_data:\n",
    "        obj_data[o] = []\n",
    "      \n",
    "      obj_data[o].append({\n",
    "        \"file\": f\"{data['camera']}/{data['name']}\",\n",
    "        \"frame\": m_frame,\n",
    "        \"time\": m_pos,\n",
    "        \"timestamp\": m_timestamp\n",
    "      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in obj_data.items():\n",
    "  obj_data[k] = sorted(v, key=lambda x: x[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OBJ_PATH_OUT.replace(\".json\", \"_flat.json\"), \"w\") as f:\n",
    "  json.dump(obj_data, f, indent=2, separators=(',',':'), sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = sorted([d for d in listdir(VIDEO_PATH) if DIR_PATTERN.search(d) is not None])[0]\n",
    "input_dir_path = path.join(VIDEO_PATH, input_dir)\n",
    "input_files = sorted([f for f in listdir(input_dir_path) if f.endswith(\"mp4\")])\n",
    "input_file_path = path.join(input_dir_path, input_files[0])\n",
    "\n",
    "vid = cv2.VideoCapture(input_file_path)\n",
    "vid.set(cv2.CAP_PROP_POS_FRAMES, 140)\n",
    "_, frame = vid.read()\n",
    "\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "image = PImage.fromarray(frame)\n",
    "\n",
    "vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "MODEL = \"Salesforce/blip-image-captioning-large\"\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(MODEL)\n",
    "model = BlipForConditionalGeneration.from_pretrained(MODEL).to(\"cuda\")\n",
    "\n",
    "input = processor(image, \"image of\", return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "out = model.generate(**input, max_length=50)\n",
    "caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "\n",
    "MODEL = \"Salesforce/blip-vqa-capfilt-large\"\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(MODEL)\n",
    "model = BlipForQuestionAnswering.from_pretrained(MODEL).to(\"cuda\")\n",
    "\n",
    "question = \"are people in the image?\"\n",
    "inputs = processor(image, question, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "out = model.generate(**inputs, max_length=32)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "images = [image] * len(questions)\n",
    "inputs = processor(images, questions, padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "out = model.generate(**inputs, max_length=32)\n",
    "answers = processor.batch_decode(out, skip_special_tokens=True)\n",
    "objs = [o for o,a in zip(OSOI, answers) if a == \"yes\"]\n",
    "print(objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "MODEL = \"microsoft/git-large-coco\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL).to(\"cuda\")\n",
    "\n",
    "input = processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "out = model.generate(**input, max_length=50)\n",
    "caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "\n",
    "MODEL = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL).to(\"cuda\")\n",
    "processor = ViTImageProcessor.from_pretrained(MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "input = processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "out = model.generate(**input, max_length=50)\n",
    "caption = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "MODEL = \"openbmb/MiniCPM-V-2\"\n",
    "\n",
    "model = AutoModel.from_pretrained(MODEL, trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "model = model.to(device='cuda', dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "msgs = [\n",
    "    {'role': 'user', 'content': \"The following image was taken during a protest.\"},\n",
    "    {'role': 'user', 'content': \"Give a short description of the image.\"},\n",
    "    {'role': 'user', 'content': \"Don't mention sports or winter.\"},\n",
    "]\n",
    "\n",
    "caption, _, _ = qa_model.chat(\n",
    "  image=image,\n",
    "  msgs=msgs,\n",
    "  max_length=32,\n",
    "  context=None,\n",
    "  tokenizer=qa_tokenizer,\n",
    "  sampling=True,\n",
    "  temperature=0.1\n",
    ")\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "answers = []\n",
    "for o in OSOI:\n",
    "  question = f'using only yes or no, are there any {o} in the image?'\n",
    "  msgs = [{'role': 'user', 'content': question}]\n",
    "\n",
    "  res, context, _ = model.chat(\n",
    "    image=image,\n",
    "    msgs=msgs,\n",
    "    context=None,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling=True,\n",
    "    temperature=0.1\n",
    "  )\n",
    "  print(res.split(',')[0].lower())\n",
    "  answers.append(res.split(',')[0].lower())\n",
    "\n",
    "objs = [o for o,a in zip(OSOI, answers) if a == \"yes\"]\n",
    "print(objs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = sorted([d for d in listdir(VIDEO_PATH) if DIR_PATTERN.search(d) is not None])[0]\n",
    "input_dir_path = path.join(VIDEO_PATH, input_dir)\n",
    "input_files = sorted([f for f in listdir(input_dir_path) if f.endswith(\"mp4\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for io_file in input_files:\n",
    "  input_file_path = path.join(input_dir_path, io_file)\n",
    "  vid = cv2.VideoCapture(input_file_path)\n",
    "  rep_frames = video_data[io_file][\"representative_frames\"]\n",
    "\n",
    "  vid.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "  for frame_data in rep_frames:\n",
    "    frameIdx = frame_data[\"index\"]\n",
    "    vid.set(cv2.CAP_PROP_POS_FRAMES, frameIdx)\n",
    "    _, frame = vid.read()\n",
    "    frame_grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "  vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for io_file in input_files:\n",
    "  input_file_path = path.join(input_dir_path, io_file)\n",
    "  vid = cv2.VideoCapture(input_file_path)\n",
    "  rep_frames = video_data[io_file][\"representative_frames\"]\n",
    "  rep_frame_idxs = [f[\"index\"] for f in rep_frames]\n",
    "\n",
    "  vid.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "  for frameIdx in range(0, frame_count):\n",
    "    _, frame = vid.read()\n",
    "    if frameIdx not in rep_frame_idxs:\n",
    "      continue\n",
    "    else:\n",
    "      frame_grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "  vid.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
