{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection / Captioning / Scene Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import scipy.fftpack as fftpack\n",
    "import torch\n",
    "\n",
    "from datetime import timedelta\n",
    "from imagehash import ImageHash\n",
    "from os import listdir, makedirs, path\n",
    "\n",
    "from PIL import Image as PImage\n",
    "\n",
    "VIDEO_DB_PATH = \"./metadata/keyframe-500/videos.json\"\n",
    "OUT_PATH = \"./metadata/objects-1152\"\n",
    "makedirs(OUT_PATH, exist_ok=True)\n",
    "\n",
    "VIDEO_PATH = \"../../vids/0801-1152\"\n",
    "DIR_PATTERN = re.compile(\"^[0-3][0-9]-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATIC_OBJS = {\n",
    "  \"cape\": 0.2,\n",
    "  \"chair\": 0.2,\n",
    "  \"fire extinguisher\": 0.3,\n",
    "  \"flag\": 0.4,\n",
    "  \"painting\": 0.2,\n",
    "  \"person\": 0.2,\n",
    "  \"pedestrian ramp\": 0.3,\n",
    "  \"sculpture\": 0.3,\n",
    "  \"stairs\": 0.4,\n",
    "  \"support column\": 0.45,\n",
    "  \"table\": 0.2,\n",
    "  \"window\": 0.2,\n",
    "}\n",
    "\n",
    "NOT_STATIC_OBJS = {\n",
    "  \"cape\": 2,\n",
    "  \"person\": 2\n",
    "}\n",
    "\n",
    "DYNAMIC_OBJS = {\n",
    "  \"bus\": 0.4,\n",
    "  \"cape\": 0.2,\n",
    "  \"car\": 0.4,\n",
    "  \"person\": 0.2,\n",
    "  \"truck\": 0.4,\n",
    "}\n",
    "\n",
    "STATIC_LABELS = sorted(STATIC_OBJS.keys())\n",
    "STATIC_THOLD = [STATIC_OBJS[k] for k in STATIC_LABELS]\n",
    "\n",
    "DYNAMIC_LABELS = sorted(DYNAMIC_OBJS.keys())\n",
    "DYNAMIC_THOLD = [DYNAMIC_OBJS[k] for k in DYNAMIC_LABELS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "CAP_MODEL_NAME = \"openbmb/MiniCPM-V-2\"\n",
    "\n",
    "CAP_COND = [\n",
    "  {'role': 'user', 'content': \"The following image was taken during a protest.\"},\n",
    "  {'role': 'user', 'content': \"Give a short description of the image.\"},\n",
    "  {'role': 'user', 'content': \"Don't mention sports or winter.\"},\n",
    "]\n",
    "\n",
    "CAP_MODEL = {\n",
    "  \"model\": AutoModel.from_pretrained(CAP_MODEL_NAME, trust_remote_code=True, torch_dtype=torch.bfloat16).to(\"cuda\", dtype=torch.bfloat16),\n",
    "  \"pre\": AutoTokenizer.from_pretrained(CAP_MODEL_NAME, trust_remote_code=True),\n",
    "  \"chat\": CAP_COND\n",
    "}\n",
    "\n",
    "CAP_MODEL[\"post\"] = CAP_MODEL[\"pre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_caption(img, model):\n",
    "  caption, _, _ = model[\"model\"].chat(\n",
    "    image=img,\n",
    "    msgs=model[\"chat\"],\n",
    "    max_length=32,\n",
    "    context=None,\n",
    "    tokenizer=model[\"pre\"],\n",
    "    sampling=True,\n",
    "    temperature=0.1\n",
    "  )\n",
    "  caption = caption[:caption.find(\".\") + 1]\n",
    "  caption = caption[:caption.find(\", possibly\")]\n",
    "  return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "\n",
    "OBJ_TARGET_SIZE = torch.Tensor([500, 500])\n",
    "OBJ_MODEL = \"google/owlv2-base-patch16-ensemble\"\n",
    "\n",
    "obj_model = Owlv2ForObjectDetection.from_pretrained(OBJ_MODEL).to(\"cuda\")\n",
    "obj_processor = Owlv2Processor.from_pretrained(OBJ_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_object_detection(img, obj_labels, obj_tholds, not_labels={}):\n",
    "  input = obj_processor(text=obj_labels, images=img, return_tensors=\"pt\").to(\"cuda\")\n",
    "  with torch.no_grad():\n",
    "    obj_out = obj_model(**input)\n",
    "\n",
    "  obj_results = obj_processor.post_process_object_detection(outputs=obj_out, target_sizes=[OBJ_TARGET_SIZE])\n",
    "  scores, labels = obj_results[0][\"scores\"], obj_results[0][\"labels\"]\n",
    "\n",
    "  all_detect_labels = [obj_labels[l.item()] for s,l in zip(scores, labels) if s > obj_tholds[l.item()]]\n",
    "\n",
    "  obj_detect_counts = {l:int(c) for l,c in zip(*np.unique(all_detect_labels, return_counts=True))}\n",
    "  obj_detect_labels = sorted(set(all_detect_labels))\n",
    "\n",
    "  for nl,nc in not_labels.items():\n",
    "    if obj_detect_counts.get(nl, 0) > nc:\n",
    "      # if not_objects are present, return their labels and counts\n",
    "      obj_detect_labels = [l for l in obj_detect_labels if l in not_labels]\n",
    "      obj_detect_counts = {l:c for l,c in obj_detect_counts.items() if l in not_labels}\n",
    "      break\n",
    "\n",
    "  return obj_detect_labels, obj_detect_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_params = dict(\n",
    "  hash_size=8,\n",
    "  highfreq_factor=4\n",
    ")\n",
    "\n",
    "def phash(im, hash_size=8, highfreq_factor=4):\n",
    "  \"\"\"from vframe: https://github.com/vframeio/vframe/blob/master/src/vframe/utils/im_utils.py#L37-L48\"\"\"\n",
    "  \"\"\"Perceptual hash rewritten from https://github.com/JohannesBuchner/imagehash/blob/master/imagehash.py#L197\"\"\"\n",
    "  wh = hash_size * highfreq_factor\n",
    "  im = cv2.resize(im, (wh, wh), interpolation=cv2.INTER_NEAREST)\n",
    "  if len(im.shape) > 2 and im.shape[2] > 1:\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "  mdct = fftpack.dct(fftpack.dct(im, axis=0), axis=1)\n",
    "  dctlowfreq = mdct[:hash_size, :hash_size]\n",
    "  med = np.median(dctlowfreq)\n",
    "  diff = dctlowfreq > med\n",
    "  return ImageHash(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_hash(h0, h_list, thold=4):\n",
    "  for h in h_list:\n",
    "    if abs(h - h0) < thold:\n",
    "      return True\n",
    "  return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Video Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VIDEO_DB_PATH, \"r\") as f:\n",
    "  video_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "input_dirs = sorted([d for d in listdir(VIDEO_PATH) if DIR_PATTERN.search(d) is not None])\n",
    "mLangs = [\"en\"]\n",
    "\n",
    "for io_dir in input_dirs:\n",
    "  output_dir_path = path.join(OUT_PATH, io_dir)\n",
    "  makedirs(output_dir_path, exist_ok=True)\n",
    "\n",
    "  input_dir_path = path.join(VIDEO_PATH, io_dir)\n",
    "  input_files = sorted([f for f in listdir(input_dir_path) if f.endswith(\"mp4\")])\n",
    "\n",
    "  processed_hashes = []\n",
    "\n",
    "  for io_file in input_files:\n",
    "    input_file_path = path.join(input_dir_path, io_file)\n",
    "    output_file_path = path.join(output_dir_path, io_file.replace(\".mp4\", \".json\"))\n",
    "\n",
    "    if io_file not in video_data:\n",
    "      print(io_file, \"not in video_data\")\n",
    "      continue\n",
    "\n",
    "    if path.isfile(output_file_path):\n",
    "      continue\n",
    "\n",
    "    print(io_dir, io_file)\n",
    "\n",
    "    vid = cv2.VideoCapture(input_file_path)\n",
    "    vw = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    vh = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_count = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = round(vid.get(cv2.CAP_PROP_FPS))\n",
    "    min_frame_diff = 1 * 60 * fps\n",
    "\n",
    "    static_frames = video_data[io_file][\"static_frames\"]\n",
    "\n",
    "    rep_frames_data = []\n",
    "    last_processed_frame = -min_frame_diff\n",
    "\n",
    "    vid.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    for frameIdx in video_data[io_file][\"representative_frames\"]:\n",
    "      if frameIdx - last_processed_frame < min_frame_diff:\n",
    "        continue\n",
    "\n",
    "      vid.set(cv2.CAP_PROP_POS_FRAMES, frameIdx)\n",
    "      _, frame = vid.read()\n",
    "      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "      image = PImage.fromarray(frame)\n",
    "\n",
    "      frame_hash = phash(frame, **hash_params)\n",
    "      if duplicate_hash(frame_hash, processed_hashes, thold=12):\n",
    "        continue\n",
    "\n",
    "      processed_hashes.append(frame_hash)\n",
    "      last_processed_frame = frameIdx\n",
    "\n",
    "      frame_data = {}\n",
    "      frame_data[\"index\"] = frameIdx\n",
    "      frame_data[\"caption\"] = {}\n",
    "      for mLang in mLangs:\n",
    "        frame_data[\"caption\"][mLang] = []\n",
    "\n",
    "      frame_data[\"caption\"][\"en\"] = run_caption(image, CAP_MODEL)\n",
    "      # TODO: translate\n",
    "\n",
    "      if frameIdx in static_frames:\n",
    "        objs, counts = run_object_detection(image, STATIC_LABELS, STATIC_THOLD, not_labels=NOT_STATIC_OBJS)\n",
    "      else:\n",
    "        objs, counts = run_object_detection(image, DYNAMIC_LABELS, DYNAMIC_THOLD)\n",
    "\n",
    "      frame_data[\"objects\"] = objs\n",
    "      frame_data[\"counts\"] = counts\n",
    "      rep_frames_data.append(frame_data)\n",
    "\n",
    "    print(len(video_data[io_file][\"representative_frames\"]), \"->\", len(rep_frames_data))\n",
    "    with open(output_file_path, \"w\") as of:\n",
    "      json.dump(rep_frames_data, of, sort_keys=True, separators=(',',':'))\n",
    "\n",
    "    vid.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Process: add to metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import re\n",
    "\n",
    "from os import listdir, makedirs, path\n",
    "\n",
    "VIDEO_DB_PATH_IN = \"./metadata/keyframe-500/videos.json\"\n",
    "\n",
    "CAPTION_PATH = \"./metadata/objects-1152\"\n",
    "VIDEO_DB_PATH_OUT = path.join(CAPTION_PATH, \"videos.json\")\n",
    "\n",
    "VIDEO_PATH = \"../../vids/0801-500\"\n",
    "IMAGE_PATH = \"../../imgs/0801-500\"\n",
    "makedirs(IMAGE_PATH, exist_ok=True)\n",
    "\n",
    "DIR_PATTERN = re.compile(\"^[0-3][0-9]-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open all caption files\n",
    "cap_data = {}\n",
    "\n",
    "input_dirs = sorted([d for d in listdir(CAPTION_PATH) if DIR_PATTERN.search(d) is not None])\n",
    "\n",
    "for io_dir in input_dirs:\n",
    "  input_dir_path = path.join(CAPTION_PATH, io_dir)\n",
    "  input_files = sorted([f for f in listdir(input_dir_path) if f.endswith(\"json\")])\n",
    "\n",
    "  for io_file in input_files:\n",
    "    input_file_path = path.join(input_dir_path, io_file)\n",
    "    video_key = io_file.replace(\"json\", \"mp4\")\n",
    "    with open(input_file_path, \"r\") as f:\n",
    "      cap_data[video_key] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VIDEO_DB_PATH_IN, \"r\") as f:\n",
    "  video_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, vdata in video_data.items():\n",
    "  if k not in cap_data:\n",
    "    print(k, \"has no caption info\")\n",
    "  else:\n",
    "    video_data[k][\"representative_frames\"] = cap_data[k]\n",
    "    video_data[k][\"representative_frames_count\"] = len(cap_data[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out frame images\n",
    "\n",
    "lastc = \"\"\n",
    "for vd in video_data.values():\n",
    "  if vd[\"camera\"] != lastc:\n",
    "    lastc = vd[\"camera\"]\n",
    "    print(vd[\"camera\"])\n",
    "  input_video_path = path.join(VIDEO_PATH, vd[\"camera\"], vd[\"name\"])\n",
    "  output_dir_path = input_video_path.replace(VIDEO_PATH, IMAGE_PATH).replace(\".mp4\", \"\")\n",
    "  makedirs(output_dir_path, exist_ok=True)\n",
    "  vd[\"representative_frames\"] = [rf for rf in vd[\"representative_frames\"] if len(rf[\"objects\"]) > 0 or len(rf[\"counts\"]) > 0]\n",
    "  if len(vd[\"representative_frames\"]) > 0:\n",
    "    vid = cv2.VideoCapture(input_video_path)\n",
    "    vid.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    for rf in vd[\"representative_frames\"]:\n",
    "      frameIdx = rf[\"index\"]\n",
    "      vid.set(cv2.CAP_PROP_POS_FRAMES, frameIdx)\n",
    "      _, frame = vid.read()\n",
    "\n",
    "      output_image_filename = f\"0000000{frameIdx}.jpg\"[-11:]\n",
    "      output_image_path = path.join(output_dir_path, output_image_filename)\n",
    "      cv2.imwrite(output_image_path, frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VIDEO_DB_PATH_OUT, \"w\") as f:\n",
    "  json.dump(video_data, f, separators=(',',':'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Process: create objects json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from os import path\n",
    "\n",
    "CAPTION_PATH = \"./metadata/objects-1152\"\n",
    "VIDEO_DB_PATH_IN = path.join(CAPTION_PATH, \"videos.json\")\n",
    "OBJ_PATH_OUT = path.join(CAPTION_PATH, \"objects.json\")\n",
    "\n",
    "with open(VIDEO_DB_PATH_IN, \"r\") as f:\n",
    "  video_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp(mpos, seek):\n",
    "  ts0, pos0 = seek[0]\n",
    "  ts1, pos1 = seek[-1]\n",
    "  for ts,pos in seek[1:]:\n",
    "    if pos >= mpos:\n",
    "      ts1, pos1 = ts, pos\n",
    "      break\n",
    "    else:\n",
    "      ts0, pos0 = ts, pos\n",
    "  if pos0 == pos1 and int(mpos) == int(pos1) and mpos > pos1:\n",
    "    pos1 = mpos\n",
    "  return ((mpos - pos0) / (pos1 - pos0)) * (ts1 - ts0) + ts0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files/Frames/Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = []\n",
    "frame_data = []\n",
    "obj_data = {}\n",
    "\n",
    "def has_objects(frame_data):\n",
    "  return len(frame_data[\"objects\"]) > 0 or len(frame_data[\"counts\"]) > 0\n",
    "\n",
    "for vid,data in video_data.items():\n",
    "  m_path = f\"{data['camera']}/{data['name']}\"\n",
    "  m_file_key = len(file_data)\n",
    "  file_data.append(m_path)\n",
    "\n",
    "  # export frame images\n",
    "  input_video_path = path.join(VIDEO_PATH, data[\"camera\"], data[\"name\"])\n",
    "  vid = cv2.VideoCapture(input_video_path)\n",
    "  vid.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "  output_dir_path = path.join(IMAGE_PATH, data[\"camera\"])\n",
    "  makedirs(output_dir_path, exist_ok=True)\n",
    "\n",
    "  data[\"representative_frames\"] = [f for f in data[\"representative_frames\"] if has_objects(f)]\n",
    "  for f in data[\"representative_frames\"]:\n",
    "    m_frame = f[\"index\"]\n",
    "    m_pos = f[\"index\"] / data[\"fps\"]\n",
    "    m_timestamp = get_timestamp(m_pos, data[\"seek\"])\n",
    "    m_caption = f[\"caption\"][\"en\"].replace(\"The image shows \", \"\")\n",
    "    m_counts = f[\"counts\"]\n",
    "    m_frame_key = len(frame_data)\n",
    "    frame_data.append({\n",
    "      \"file\": m_file_key,\n",
    "      \"frame\": m_frame,\n",
    "      \"time\": round(m_pos, 5),\n",
    "      \"timestamp\": round(m_timestamp, 5),\n",
    "      \"caption\": m_caption,\n",
    "      \"counts\": m_counts\n",
    "    })\n",
    "    # export frame\n",
    "    vid.set(cv2.CAP_PROP_POS_FRAMES, m_frame)\n",
    "    _, frame = vid.read()\n",
    "\n",
    "    output_image_filename = f\"{int(m_timestamp)}.jpg\"\n",
    "    output_image_path = path.join(output_dir_path, output_image_filename)\n",
    "    cv2.imwrite(output_image_path, frame)\n",
    "\n",
    "    for o in f[\"objects\"]:\n",
    "      if o not in obj_data:\n",
    "        obj_data[o] = []\n",
    "      obj_data[o].append({\n",
    "        \"frame\": m_frame_key,\n",
    "        \"timestamp\": m_timestamp\n",
    "      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in obj_data.items():\n",
    "  sorted_by_ts = sorted(v, key=lambda x: x[\"timestamp\"])\n",
    "  obj_data[k] = [x[\"frame\"] for x in sorted_by_ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data = {\n",
    "  \"files\": file_data,\n",
    "  \"frames\": frame_data,\n",
    "  \"objects\": obj_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OBJ_PATH_OUT, \"w\") as f:\n",
    "  json.dump(out_data, f, separators=(',',':'), sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import display, Image\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "\n",
    "MODEL = \"google/owlv2-base-patch16-ensemble\"\n",
    "\n",
    "model = Owlv2ForObjectDetection.from_pretrained(MODEL).to(\"cuda\")\n",
    "processor = Owlv2Processor.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = \"../../vids/0801-500\"\n",
    "input_dir = sorted([d for d in listdir(VIDEO_PATH) if DIR_PATTERN.search(d) is not None])[0]\n",
    "input_dir_path = path.join(VIDEO_PATH, input_dir)\n",
    "input_files = sorted([f for f in listdir(input_dir_path) if f.endswith(\"mp4\")])\n",
    "input_file_path = path.join(input_dir_path, input_files[0])\n",
    "\n",
    "vid = cv2.VideoCapture(input_file_path)\n",
    "vid.set(cv2.CAP_PROP_POS_FRAMES, 80)\n",
    "_, frame = vid.read()\n",
    "\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "image = PImage.fromarray(frame)\n",
    "\n",
    "vid.release()\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIND_LABELS = sorted(OBS_FIND.keys())\n",
    "COUNT_LABELS = sorted(OBS_COUNT.keys())\n",
    "\n",
    "FIND_THOLD = [OBS_FIND[k] for k in FIND_LABELS]\n",
    "COUNT_THOLD = [OBS_COUNT[k] for k in COUNT_LABELS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "target_sizes = torch.Tensor([500, 500])\n",
    "inputs = processor(text=FIND_LABELS, images=image, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)\n",
    "\n",
    "results = processor.post_process_object_detection(outputs=outputs, target_sizes=[target_sizes])\n",
    "scores, labels = results[0][\"scores\"], results[0][\"labels\"]\n",
    "\n",
    "result_labels = [FIND_LABELS[l.item()] for s,l in zip(scores, labels) if s > FIND_THOLD[l.item()]]\n",
    "\n",
    "set(result_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "MODEL = \"Salesforce/blip-image-captioning-large\"\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(MODEL)\n",
    "model = BlipForConditionalGeneration.from_pretrained(MODEL).to(\"cuda\")\n",
    "\n",
    "input = processor(image, \"image of\", return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "out = model.generate(**input, max_length=50)\n",
    "caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "\n",
    "MODEL = \"Salesforce/blip-vqa-capfilt-large\"\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(MODEL)\n",
    "model = BlipForQuestionAnswering.from_pretrained(MODEL).to(\"cuda\")\n",
    "\n",
    "question = \"are people in the image?\"\n",
    "inputs = processor(image, question, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "out = model.generate(**inputs, max_length=32)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "images = [image] * len(questions)\n",
    "inputs = processor(images, questions, padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "out = model.generate(**inputs, max_length=32)\n",
    "answers = processor.batch_decode(out, skip_special_tokens=True)\n",
    "objs = [o for o,a in zip(OBSOI, answers) if a == \"yes\"]\n",
    "print(objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "\n",
    "MODEL = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL).to(\"cuda\")\n",
    "processor = ViTImageProcessor.from_pretrained(MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "input = processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "out = model.generate(**input, max_length=50)\n",
    "caption = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "MODEL = \"openbmb/MiniCPM-V-2\"\n",
    "\n",
    "model = AutoModel.from_pretrained(MODEL, trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "model = model.to(device='cuda', dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "msgs = [\n",
    "    {'role': 'user', 'content': \"The following image was taken during a protest.\"},\n",
    "    {'role': 'user', 'content': \"Give a short description of the image.\"},\n",
    "    {'role': 'user', 'content': \"Don't mention sports or winter.\"},\n",
    "]\n",
    "\n",
    "caption, _, _ = qa_model.chat(\n",
    "  image=image,\n",
    "  msgs=msgs,\n",
    "  max_length=32,\n",
    "  context=None,\n",
    "  tokenizer=qa_tokenizer,\n",
    "  sampling=True,\n",
    "  temperature=0.1\n",
    ")\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "answers = []\n",
    "for o in OBSOI:\n",
    "  question = f'using only yes or no, are there any {o} in the image?'\n",
    "  msgs = [{'role': 'user', 'content': question}]\n",
    "\n",
    "  res, context, _ = model.chat(\n",
    "    image=image,\n",
    "    msgs=msgs,\n",
    "    context=None,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling=True,\n",
    "    temperature=0.1\n",
    "  )\n",
    "  print(res.split(',')[0].lower())\n",
    "  answers.append(res.split(',')[0].lower())\n",
    "\n",
    "objs = [o for o,a in zip(OSOI, answers) if a == \"yes\"]\n",
    "print(objs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = sorted([d for d in listdir(VIDEO_PATH) if DIR_PATTERN.search(d) is not None])[0]\n",
    "input_dir_path = path.join(VIDEO_PATH, input_dir)\n",
    "input_files = sorted([f for f in listdir(input_dir_path) if f.endswith(\"mp4\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for io_file in input_files:\n",
    "  input_file_path = path.join(input_dir_path, io_file)\n",
    "  vid = cv2.VideoCapture(input_file_path)\n",
    "  rep_frames = video_data[io_file][\"representative_frames\"]\n",
    "\n",
    "  vid.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "  for frame_data in rep_frames:\n",
    "    frameIdx = frame_data[\"index\"]\n",
    "    vid.set(cv2.CAP_PROP_POS_FRAMES, frameIdx)\n",
    "    _, frame = vid.read()\n",
    "    frame_grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "  vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for io_file in input_files:\n",
    "  input_file_path = path.join(input_dir_path, io_file)\n",
    "  vid = cv2.VideoCapture(input_file_path)\n",
    "  rep_frames = video_data[io_file][\"representative_frames\"]\n",
    "  rep_frame_idxs = [f[\"index\"] for f in rep_frames]\n",
    "\n",
    "  vid.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "  for frameIdx in range(0, frame_count):\n",
    "    _, frame = vid.read()\n",
    "    if frameIdx not in rep_frame_idxs:\n",
    "      continue\n",
    "    else:\n",
    "      frame_grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "  vid.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
